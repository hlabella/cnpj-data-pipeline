# ğŸ‡§ğŸ‡· CNPJ Data Pipeline

Pipeline modular para processar dados CNPJ da Receita Federal. Processa 60+ milhÃµes de empresas brasileiras com suporte a mÃºltiplos bancos de dados.

**[English version below](#-cnpj-data-pipeline-english)** ğŸ‘‡

## CaracterÃ­sticas Principais

- **Arquitetura Modular**: SeparaÃ§Ã£o clara de responsabilidades com camada de abstraÃ§Ã£o de banco de dados
- **Multi-Banco**: PostgreSQL totalmente suportado, com placeholders para MySQL, BigQuery e SQLite
- **Processamento Inteligente**: AdaptaÃ§Ã£o automÃ¡tica da estratÃ©gia baseada em recursos disponÃ­veis
- **Downloads Paralelos**: EstratÃ©gia configurÃ¡vel para otimizar velocidade de download
- **Processamento Incremental**: Rastreamento de arquivos processados para evitar duplicaÃ§Ãµes
- **Performance Otimizada**: OperaÃ§Ãµes bulk eficientes com tratamento de conflitos
- **ConfiguraÃ§Ã£o Simples**: Setup interativo + variÃ¡veis de ambiente

## InÃ­cio RÃ¡pido

```bash
# Clone o repositÃ³rio
git clone https://github.com/cnpj-chat/cnpj-data-pipeline
cd cnpj-data-pipeline

# OpÃ§Ã£o 1: Setup interativo (recomendado)
make setup

# OpÃ§Ã£o 2: Setup manual
make install
make env
# Editar .env com suas configuraÃ§Ãµes
make run
```

### Com Docker

```bash
# Iniciar PostgreSQL
make docker-db

# Executar pipeline
make docker-run

# Parar containers
make docker-stop

# Limpar tudo (containers + volumes)
make docker-clean
```

## ConfiguraÃ§Ã£o (.env)

### Essencial

```bash
# Database
DATABASE_BACKEND=postgresql

# Future support
# DATABASE_BACKEND=mysql
# DATABASE_BACKEND=bigquery
# DATABASE_BACKEND=sqlite

# Performance
BATCH_SIZE=50000              # Batch size
MAX_MEMORY_PERCENT=80         # Max memory usage
```

### OtimizaÃ§Ãµes

```bash
# Downloads
DOWNLOAD_STRATEGY=parallel    # ou sequential
DOWNLOAD_WORKERS=4           # Para downloads paralelos
KEEP_DOWNLOADED_FILES=false  # true economiza bandwidth em re-execuÃ§Ãµes

# DiretÃ³rios
TEMP_DIR=./temp              # Para arquivos temporÃ¡rios
```

## Agendamento Mensal

A Receita atualiza os dados mensalmente. Configure execuÃ§Ã£o automÃ¡tica:

```bash
# Linux/Mac (cron) - dia 5 Ã s 2h
0 2 5 * * cd /path/to/cnpj-pipeline && make run >> logs/scheduled.log 2>&1

# Ou use o scheduler da sua plataforma (Task Scheduler, Kubernetes CronJob, etc.)
```

## Arquitetura

```
cnpj-data-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py            # Auto-detecÃ§Ã£o de recursos
â”‚   â”œâ”€â”€ downloader.py        # Download com retry
â”‚   â”œâ”€â”€ processor.py         # Parsing otimizado
â”‚   â”œâ”€â”€ download_strategies/ # Sequential/Parallel
â”‚   â””â”€â”€ database/            # AbstraÃ§Ã£o PostgreSQL
â”œâ”€â”€ main.py                  # Entry point
â”œâ”€â”€ setup.py                 # Assistente interativo
â””â”€â”€ Makefile                 # Comandos Ãºteis
```

## Fluxo de Processamento

1. **Descoberta**: Localiza dados mais recentes da Receita
2. **Download**: Baixa ZIPs com retry automÃ¡tico
3. **Processamento**: Parse otimizado dos CSVs
4. **Carga**: Bulk insert no PostgreSQL
5. **Rastreamento**: Marca arquivos processados

## Performance

| Sistema | MemÃ³ria | Tempo Estimado |
|---------|---------|----------------|
| VPS bÃ¡sico | 4GB | ~6 horas |
| Servidor padrÃ£o | 16GB | ~2 horas |
| High-end | 64GB+ | ~1 hora |

## Comandos Ãšteis

```bash
make logs           # Ver logs recentes
make clean          # Limpar temporÃ¡rios
make clean-data     # Remover downloads (pede confirmaÃ§Ã£o)
```

## Desenvolvimento

### Adicionando Novo Backend

1. Criar adapter em `src/database/seu_banco.py`
2. Implementar mÃ©todos abstratos de `DatabaseAdapter`
3. Registrar no factory em `src/database/factory.py`
4. Criar arquivo de requirements em `requirements/seu_banco.txt`

---

# ğŸ‡§ğŸ‡· CNPJ Data Pipeline (English)

Modular pipeline for processing Brazilian CNPJ (company registry) data. Processes 60+ million companies with optimized PostgreSQL support.

## Key Features

- **Smart Processing**: Auto-adapts to available resources
- **Advanced Filtering**: Filter by state, CNAE, and company size via CLI
- **Parallel Downloads**: Configurable strategy for optimized download speed
- **Incremental**: Tracks processed files
- **Optimized**: Efficient bulk operations
- **Easy Config**: Interactive setup + env vars

## Quick Start

```bash
# Clone repository
git clone https://github.com/cnpj-chat/cnpj-data-pipeline
cd cnpj-data-pipeline

# Option 1: Interactive setup (recommended)
make setup

# Option 2: Manual setup
make install
make env
# Edit .env with your settings
make run
```

### With Docker

```bash
# Start PostgreSQL
make docker-db

# Run pipeline
make docker-run

# Stop containers
make docker-stop

# Clean everything (containers + volumes)
make docker-clean
```


## Configuration (.env)

### Essential

```bash
# Database
DATABASE_BACKEND=postgresql

# Future support
# DATABASE_BACKEND=mysql
# DATABASE_BACKEND=bigquery
# DATABASE_BACKEND=sqlite

# Performance
BATCH_SIZE=50000              # Batch size
MAX_MEMORY_PERCENT=80         # Max memory usage
```

### Optimizations

```bash
# Downloads
DOWNLOAD_STRATEGY=parallel    # or sequential
DOWNLOAD_WORKERS=4           # For parallel downloads
KEEP_DOWNLOADED_FILES=false  # true saves bandwidth on re-runs

# Directories
TEMP_DIR=./temp              # For temporary files
```

## Monthly Scheduling

Government updates data monthly. Set up automatic execution:

```bash
# Linux/Mac (cron) - 5th day at 2 AM
0 2 5 * * cd /path/to/cnpj-pipeline && make run >> logs/scheduled.log 2>&1

# Or use your platform's scheduler (Task Scheduler, Kubernetes CronJob, etc.)
```

## Architecture

```
cnpj-data-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py            # Resource auto-detection
â”‚   â”œâ”€â”€ downloader.py        # Download with retry
â”‚   â”œâ”€â”€ processor.py         # Optimized parsing
â”‚   â”œâ”€â”€ filters/             # Filter system
â”‚   â”œâ”€â”€ download_strategies/ # Sequential/Parallel
â”‚   â””â”€â”€ database/            # PostgreSQL abstraction
â”œâ”€â”€ main.py                  # Entry point
â”œâ”€â”€ setup.py                 # Interactive wizard
â””â”€â”€ Makefile                 # Useful commands
```

## Processing Flow

1. **Discovery**: Finds latest government data
2. **Download**: Gets ZIPs with auto-retry
3. **Filtering**: Applies selected filters
4. **Processing**: Optimized CSV parsing
5. **Loading**: Bulk insert to PostgreSQL
6. **Tracking**: Marks processed files

## Performance

| System | Memory | Estimated Time |
|--------|--------|----------------|
| Basic VPS | 4GB | ~6 hours |
| Standard server | 16GB | ~2 hours |
| High-end | 64GB+ | ~1 hour |

## Useful Commands

```bash
make logs           # View recent logs
make clean          # Clean temporary files
make clean-data     # Remove downloads (asks confirmation)
```

---

Made with â¤ï¸ for the Brazilian tech community
